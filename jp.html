<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 日本語 | Chen-Chieh (Jacky) Liao </title> <meta name="author" content="Chen-Chieh (Jacky) Liao"> <meta name="description" content="廖振傑"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://liaochenchieh.github.io/jp"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Chen-Chieh</span> (Jacky) Liao </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/jp">日本語 <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Chen-Chieh</span> (Jacky) Liao </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/prof_pic.jpg?2843fb5b6ac5b0068f164dfcea6884e8" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p> <i class="fa-solid fa-location-dot iconlocation"></i> Tokyo, Japan</p> </div> <div class="contact-icons social"> <a href="mailto:%6C%69%61%6F%63%68%65%6E%63%68%69%65%68@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/liaochenchieh" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/chen-chieh-liao" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://orcid.org/0000-0002-9850-2468" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://www.researchgate.net/profile/Chen-Chieh-Liao-2182744588/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://scholar.google.com/citations?user=jYJJazcAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> </div> </div> <div class="clearfix"> <p>こんにちは、廖(りゃお)振傑です。現在は<a href="https://www.isct.ac.jp/" rel="external nofollow noopener" target="_blank">東京科学大学</a>情報理工学院で特任助教を務めています。専門はヒューマンコンピュータインタラクション、AR/VR/XR、ビジュアルコンピューティング、機械学習です。</p> </div> <article> <div class="cv"> <a class="anchor" id="work"></a> <div class="card mt-3 p-3"> <h3 class="card-title font-weight-medium">Work</h3> <div> <ul class="card-text font-weight-light list-group list-group-flush"> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <table class="table-cv"> <tbody> <tr> <td> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2026.01 - Present </span> </td> </tr> </tbody> </table> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="https://www.isct.ac.jp/en" rel="external nofollow noopener" target="_blank">Institute of Science Tokyo</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">Research Assistant Professor</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic"></h6> <ul class="items"> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <table class="table-cv"> <tbody> <tr> <td> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2025.06 - Present </span> </td> </tr> </tbody> </table> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="https://www.u-tokyo.ac.jp/en/" rel="external nofollow noopener" target="_blank">The University of Tokyo</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">Guest Researcher</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic"></h6> <ul class="items"> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <table class="table-cv"> <tbody> <tr> <td> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2022.02 - Present </span> </td> </tr> </tbody> </table> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="">Rinji Advice Co., Ltd.</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">Data Scientist</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic"></h6> <ul class="items"> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <table class="table-cv"> <tbody> <tr> <td> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2025.04 - 2025.12 </span> </td> </tr> </tbody> </table> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="https://www.isct.ac.jp/en" rel="external nofollow noopener" target="_blank">Institute of Science Tokyo</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">Researcher</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic"></h6> <ul class="items"> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <table class="table-cv"> <tbody> <tr> <td> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2024.01 - 2025.08 </span> </td> </tr> </tbody> </table> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="https://www.cygames.co.jp/en/" rel="external nofollow noopener" target="_blank">Cygames, Inc.</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">Associate Researcher</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic"></h6> <ul class="items"> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <table class="table-cv"> <tbody> <tr> <td> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2023.04 - 2025.03 </span> </td> </tr> </tbody> </table> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="https://www.kmd.keio.ac.jp/" rel="external nofollow noopener" target="_blank">Keio University</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">Visiting Researcher</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic"></h6> <ul class="items"> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <table class="table-cv"> <tbody> <tr> <td> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2022.09 - 2023.03 </span> </td> </tr> </tbody> </table> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="https://www.navercloudcorp.com/lang/en/" rel="external nofollow noopener" target="_blank">NAVER Cloud</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">AI Researcher</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic"></h6> <ul class="items"> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <table class="table-cv"> <tbody> <tr> <td> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2022.07 - 2024.02 </span> </td> </tr> </tbody> </table> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="https://acesinc.co.jp/" rel="external nofollow noopener" target="_blank">ACES, Inc.</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">Algorithm Engineer</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic"></h6> <ul class="items"> </ul> </div> </div> </li> </ul> </div> </div> <a class="anchor" id="education"></a> <div class="card mt-3 p-3"> <h3 class="card-title font-weight-medium">Education</h3> <div> <ul class="card-text font-weight-light list-group list-group-flush"> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <table class="table-cv"> <tbody> <tr> <td> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2022.04 - 2025.03 </span> </td> </tr> <tr> <td> <p class="location"> <i class="fa-solid fa-location-dot iconlocation"></i> Tokyo, Japan </p> </td> </tr> </tbody> </table> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="https://www.isct.ac.jp/en" rel="external nofollow noopener" target="_blank">Institute of Science Tokyo</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">Ph.D. in Computer Science</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic"></h6> <ul class="items"> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <table class="table-cv"> <tbody> <tr> <td> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2020.04 - 2022.03 </span> </td> </tr> <tr> <td> <p class="location"> <i class="fa-solid fa-location-dot iconlocation"></i> Tokyo, Japan </p> </td> </tr> </tbody> </table> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="https://www.titech.ac.jp/english" rel="external nofollow noopener" target="_blank">Institute of Science Tokyo (Tokyo Institute of Technology)</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">M.S. in Computer Science</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic"></h6> <ul class="items"> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <table class="table-cv"> <tbody> <tr> <td> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2018.09 - 2019.08 </span> </td> </tr> <tr> <td> <p class="location"> <i class="fa-solid fa-location-dot iconlocation"></i> Zürich, Switzerland </p> </td> </tr> </tbody> </table> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="https://ethz.ch/en.html" rel="external nofollow noopener" target="_blank">ETH Zürich</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">Exchange Program in Department of Computer Science</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic"></h6> <ul class="items"> </ul> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <table class="table-cv"> <tbody> <tr> <td> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2016.04 - 2020.03 </span> </td> </tr> <tr> <td> <p class="location"> <i class="fa-solid fa-location-dot iconlocation"></i> Tokyo, Japan </p> </td> </tr> </tbody> </table> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="https://www.titech.ac.jp/english" rel="external nofollow noopener" target="_blank">Institute of Science Tokyo (Tokyo Institute of Technology)</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">B.E. in Computer Science</h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic"></h6> <ul class="items"> </ul> </div> </div> </li> </ul> </div> </div> <a class="anchor" id="grants"></a> <div class="card mt-3 p-3"> <h3 class="card-title font-weight-medium">Grants</h3> <div> <ul class="card-text font-weight-light list-group list-group-flush"> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2023.04 </span> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="https://www.jsps.go.jp/english/" rel="external nofollow noopener" target="_blank">Japan Society for the Promotion of Science</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem"></h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">Research Fellowships for Young Scientists (DC2)</h6> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2022.04 </span> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="https://www.mext.go.jp/en/" rel="external nofollow noopener" target="_blank">Ministry of Education, Culture, Sports, Science and Technology (MEXT)</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem"></h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">Japanese Government Scholarship</h6> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2020.04 </span> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="https://moritani-scholarship.or.jp/" rel="external nofollow noopener" target="_blank">Morinati Scholarship Foundation</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem"></h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">Privately Funded Scholarship</h6> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2018.09 </span> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="http://www.heyning-roelli-stiftung.ch/" rel="external nofollow noopener" target="_blank">Heyning-Roelli Foundation</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem"></h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">Privately Funded Scholarship</h6> </div> </div> </li> <li class="list-group-item"> <div class="row"> <div class="col-xs-2 cl-sm-2 col-md-2 text-center date-column"> <span class="badge font-weight-bold danger-color-dark text-uppercase align-middle" style="min-width: 75px"> 2018.09 </span> </div> <div class="col-xs-10 cl-sm-10 col-md-10 mt-2 mt-md-0"> <h6 class="title font-weight-bold ml-1 ml-md-4"> <a href="https://www.isct.ac.jp/en/003/fund/" rel="external nofollow noopener" target="_blank">Institute of Science Tokyo (Tokyo Institute of Technology)</a> </h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem"></h6> <h6 class="ml-1 ml-md-4" style="font-size: 0.95rem; font-style: italic">University Funed Scholarship</h6> </div> </div> </li> </ul> </div> </div> <a class="anchor" id="publications"></a> <div class="card mt-3 p-3"> <h3 class="card-title font-weight-medium">Publications</h3> <div> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div id="10.1145/3706598.3713465" class="col-sm-10"> <div class="title">PiaMuscle: Improving Piano Skill Acquisition by Cost-effectively Estimating and Visualizing Activities of Miniature Hand Muscles</div> <div class="author"> Ruofan Liu, Yichen Peng, Takanori Oku, <em>Chen-Chieh Liao</em>, Erwin Wu, Shinichi Furuya, and Hideki Koike </div> <div class="periodical"> <em>In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems</em>, , Oct 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3706598.3713465" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Understanding neuromusculoskeletal mechanisms significantly impacts skill specialization and proficiency. While existing methods can infer large muscle activities during gross motor movements, the estimation of dexterous motor control involving miniature muscles remains underexplored. Targeting the coordinated hand muscles in advanced piano performance, we learn spatiotemporal discrete representations of electromyography (EMG) data and hand postures utilizing a multimodal dataset. Subsequently, we train a precise and cost-effective neural network model. Based on this model, PiaMuscle is introduced to investigate if visualizing muscle activities during piano training enhances piano performance. Quantitative and qualitative results of a user study with highly skilled professional pianists demonstrate that PiaMuscle provides reliable muscle activation data to support and optimize force control. Our research underscores the potential of a naturalistic workflow to estimate small muscles’ activities from readily accessible human-centric information and more accurately when combined with tool-centric data, thereby enhancing skill acquisition.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="10930710" class="col-sm-10"> <div class="title">ShiftingGolf: Gross Motor Skill Correction using Redirection in VR</div> <div class="author"> <em>Chen-Chieh Liao</em>, Zhihao Yu, and Hideki Koike </div> <div class="periodical"> <em>IEEE Transactions on Visualization and Computer Graphics</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/TVCG.2025.3549170" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Sports performance is often hindered by unintentional habits, particularly in golf, where achieving a consistent and correct swing is crucial yet challenging due to ingrained swing path habits. This study explores redirection approaches in virtual reality (VR) to correct golfers’ swing paths through strategic ball shifting. By initiating a forward ball shift just before impact, we aim to prompt golfers to react and modify their swing motion, thereby eliminating undesirable swing habits. Building on recent research, our VR-based methods incorporate a gradual transformation of visuomotor associations to enhance motor skill learning. In this study, we develop three ball shift patterns, including a novel pattern that employs gradual ball shifts with interspersed normal conditions, designed to retain learning effects post-training. A preliminary study, including expert interviews, assesses the feasibility of various ball-shifting directions. Subsequently, a comprehensive user study measures the learning effects across different ball shift modes. The results indicate that our proposed redirection mode effectively corrects swing paths and yields a sustained learning effect.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="10.1007/978-3-031-91907-7_14" class="col-sm-10"> <div class="title">Diffusion-Based Synthetic Dataset Generation for Egocentric 3D Human Pose Estimation</div> <div class="author"> Kyohei Hayakawa, Dong-Hyun Hwang, <em>Chen-Chieh Liao</em>, and Hideki Koike </div> <div class="periodical"> <em>In Computer Vision – ECCV 2024 Workshops</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Egocentric 3D human pose estimation, which estimates an individual’s 3D pose from a camera attached to a part of their body, operates within a specialized camera domain to capture the entire body. This specialization makes it challenging to collect real-world training data due to the difficulty in acquiring diverse and accurately labeled data from the egocentric perspective. Consequently, most existing methods rely on synthetic data for training, which increases the mean joint error when applied to real-world images due to the domain gap. Some works have addressed this issue by generating pseudo-labels from synchronized real egocentric and exocentric images and using them for training. However, this approach is costly in terms of data collection, making it difficult to scale and apply to other camera setups. In this work, we propose a novel method that employs a diffusion model with ControlNet to generate real-world-like images, thereby reducing the domain gap. The proposed method relies on synthetic data, which is easy to acquire, and a small amount of text-captioned real-world data. This method easily applies to estimating egocentric 3D human poses across various camera setups. Experiments with two different camera setups demonstrated that models trained with images generated by the proposed method improve accuracy with real-world data. Specifically, the PA-MPJPE of the Mo2Cap2 model improved by 8.9% in the SceneEgo test set and by 3.4% in the GlobalEgoMocap test set.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="10765309" class="col-sm-10"> <div class="title">Novel Sensing Methods for Vocal Technique Analysis: Evaluation on Electromyography and Ultrasonography</div> <div class="author"> Kanyu Chen, Erwin Wu, Daichi Saito, Yichen Peng, <em>Chen-Chieh Liao</em>, Akira Kato, Hideki Koike, and Kai Kunze </div> <div class="periodical"> <em>In 2024 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ISMAR-Adjunct64951.2024.00034" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Controlling vocal cord muscles is crucial for vocal performance and training, yet it is challenging to measure. This paper introduces electromyography (EMG) and ultrasonography to detect vocal muscle activity and assess pitch training skills. A pre-experiment with 16 participants analyzed muscle control discrepancies among singers of different skill levels. A subsequent user study with 12 participants evaluated EMG and ultrasonography feedback effectiveness. Findings indicate that EMG offers better temporal stability representation, while ultrasonography provides intuitive visual feedback on vocal cord activity. Both methods show potential in enhancing vocal control, offering insights for designing effective and non-invasive vocal training systems.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="10494108" class="col-sm-10"> <div class="title">ARpenSki: Augmenting Ski Training with Direct and Indirect Postural Visualization</div> <div class="author"> Takashi Matsumoto, Erwin Wu, <em>Chen-Chieh Liao</em>, and Hideki Koike </div> <div class="periodical"> <em>In 2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)</em>, Mar 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/VR58804.2024.00024" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Alpine skiing is a popular winter sport, and several systems have been proposed to enhance training and improve efficiency. However, many existing systems rely on simulation-based environments, which suffer from drawbacks such as a gap between real skiing and the lack of body ownership. To address these limitations, we present ARpenSki, a novel augmented reality (AR) ski training system that employs a see-through head mounted display (HMD) to deliver augmented visual training cues that may be applied on real slopes. The proposed AR system provides a transparent view of the lower half of the field of vision, where we implemented three different AR-based direct and indirect postural visualization methods. We conducted an user study to investigate the influence of different visual cues in the AR environment. Our results indicate that a simple AR visualization of the user’s spine (Figure 1.2) yields the most favorable training performance, surpassing conventional visualizations by 7% improvement in the user’s posture. Building upon these promising findings, we further tested our system on real slopes and showed the potential of a real AR skiing application.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="10.1145/3606038.3616151" class="col-sm-10"> <div class="title">SkiTech: An Alpine Skiing and Snowboarding Dataset of 3D Body Pose, Sole Pressure, and Electromyography</div> <div class="author"> Erwin Wu, Takashi Matsumoto, <em>Chen-Chieh Liao</em>, Ruofan Liu, Hidetaka Katsuyama, Yuki Inaba, Noriko Hakamada, Yusuke Yamamoto, Yusuke Ishige, and Hideki Koike </div> <div class="periodical"> <em>In Proceedings of the 6th International Workshop on Multimedia Content Analysis in Sports</em>, Ottawa ON, Canada, Mar 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3606038.3616151" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Effective analysis of skills requires high-quality, multi-modal datasets, especially in the field of artificial intelligence. However, creating such datasets for extreme sports, such as alpine skiing, can be challenging due to environmental constraints. Optical and wearable sensors may not perform optimally under diverse lighting, weather, and terrain conditions. To address these challenges, we present a comprehensive skiing/snowboarding dataset using a professional motor-based simulator. Using the realistic simulator, it is easy to obtain different types of data with a small domain gap between real-world data. Common data for skill analysis are collected, including camera images, 3D body pose, sole pressure, and leg electromyography, from athletes of different levels. Another key aspect is the comparison of cross-modal baselines, highlighting the versatility of the data across modalities. In addition, a real-world pilot test is conducted to assess the practical applicability and data robustness.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="10316487" class="col-sm-10"> <div class="title">PianoSyncAR: Enhancing Piano Learning through Visualizing Synchronized Hand Pose Discrepancies in Augmented Reality</div> <div class="author"> Ruofan Liu, Erwin Wu, <em>Chen-Chieh Liao</em>, Hayato Nishioka, Shinichi Furuya, and Hideki Koike </div> <div class="periodical"> <em>In 2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ISMAR59233.2023.00101" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Motor skill acquisition involves learning from spatiotemporal discrepancies between target and self-generated motions. However, in dexterous skills with numerous degrees of freedom, understanding and correcting these motor errors are challenging. This issue becomes crucial for experienced individuals who seek for mastering and sophisticating their skills, where even subtle errors need to be minimized. To enable efficient optimization of body posture in piano learning, we present PianoSyncAR, an augmented reality system that superimposes the time-varying complex hand postures of a teacher over the hand of a learner. Through a user study with 12 pianists, we demonstrate several advantages of the proposed system over conventional tablet-screen, which implicate the potential of AR training as a complementary tool for video-based skill learning in piano playing.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="10.1145/3588028.3603679" class="col-sm-10"> <div class="title">Content-Preserving Motion Stylization using Variational Autoencoder</div> <div class="author"> <em>Chen-Chieh Liao</em>, Jong-Hwan Kim, Hideki Koike, and Dong-Hyun Hwang </div> <div class="periodical"> <em>In ACM SIGGRAPH 2023 Posters</em>, Los Angeles, CA, USA, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3588028.3603679" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>This work proposes a motion style transfer network that transfers motion style between different motion categories using variational autoencoders. The proposed network effectively transfers style among various motion categories and can create stylized motion unseen in the dataset. The network contains a content-conditioned module to preserve the characteristic of the content motion, which is important for real applications. We implement the network with variational autoencoders, which enable us to control the intensity of the style and mix different styles to enrich the motion diversity.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="10.1145/3577190.3614106" class="col-sm-10"> <div class="title">Make Your Brief Stroke Real and Stereoscopic: 3D-Aware Simplified Sketch to Portrait Generation</div> <div class="author"> Yasheng Sun, Qianyi Wu, Hang Zhou, Kaisiyuan Wang, Tianshu Hu, <em>Chen-Chieh Liao</em>, Shio Miyafuji, Ziwei Liu, and Hideki Koike </div> <div class="periodical"> <em>In Proceedings of the 25th International Conference on Multimodal Interaction</em>, Paris, France, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3577190.3614106" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Creating the photo-realistic version of people’s sketched portraits is useful to various entertainment purposes. Existing studies only generate portraits in the 2D plane with fixed views, making the results less vivid. In this paper, we present Stereoscopic Simplified Sketch-to-Portrait (SSSP), which explores the possibility of creating Stereoscopic 3D-aware portraits from simple contour sketches by involving 3D generative models. Our key insight is to design sketch-aware constraints that can fully exploit the prior knowledge of a tri-plane-based 3D-aware generative model. Specifically, our designed region-aware volume rendering strategy and global consistency constraint further enhance detail correspondences during sketch encoding. Moreover, in order to facilitate the usage of layman users, we propose a Contour-to-Sketch module with vector quantized representations, so that easily drawn contours can directly guide the generation of 3D portraits. Extensive comparisons show that our method generates high-quality results that match the sketch. Our usability study verifies that our system is preferred by users.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="10.1145/3544549.3585705" class="col-sm-10"> <div class="title">PianoHandSync: An Alignment-based Hand Pose Discrepancy Visualization System for Piano Learning</div> <div class="author"> Ruofan Liu, Erwin Wu, <em>Chen-Chieh Liao</em>, Hayato Nishioka, Shinichi Furuya, and Hideki Koike </div> <div class="periodical"> <em>In Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems</em>, Hamburg, Germany, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3544549.3585705" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Video-based lessons are becoming a popular way for distance piano education. However, limited by the fixed camera angle, a video is difficult to tell precise 3D hand posture, which is one of the most essential factors for learning piano. This paper presents a visualization system providing the intuitive discrepancy of hand postures in two piano performance videos. Through a motion capture system, the estimated 3D postures are visualized and discrepancies based on distinct metrics are displayed, integrated with modular functions assisting skill acquisition. A pilot study proves that the proposed visualization can be a supplementary means for only video-based lessons in terms of correcting hand postures and fingering.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="kobayashi2023motioncapturedatasetpractical" class="col-sm-10"> <div class="title">Motion Capture Dataset for Practical Use of AI-based Motion Editing and Stylization</div> <div class="author"> Makito Kobayashi, <em>Chen-Chieh Liao</em>, Keito Inoue, Sentaro Yojima, and Masafumi Takahashi </div> <div class="periodical"> Oct 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div id="10.1145/3582700.3582710" class="col-sm-10"> <div class="title">AI Coach: A Motor Skill Training System using Motion Discrepancy Detection</div> <div class="author"> <em>Chen-Chieh Liao</em>, Dong-Hyun Hwang, Erwin Wu, and Hideki Koike </div> <div class="periodical"> <em>In Proceedings of the Augmented Humans International Conference 2023</em>, Glasgow, United Kingdom, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3582700.3582710" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Spatial and temporal clues found in a professional’s motion are essential for designing a training system for learning a motor skill. We investigate the potential of using neural networks to learn spatial and temporal features of advanced players in sports and to detect the fine-grained differences between motions. As a training system, we implement an AI Coach prototype application that finds the differences between two input motions and visualizes a recommendation motion for the users to correct their forms. In the user study, we investigate the effects of the proposed AI Coach and discuss the findings based on quantitative questionnaires and qualitative interviews. In the study, the proposed system can help the user better understand the difference between them and the coach. The study also reveals the necessity of coaching beginners in the early learning phases.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="9913343" class="col-sm-10"> <div class="title">AI Golf: Golf Swing Analysis Tool for Self-Training</div> <div class="author"> <em>Chen-Chieh Liao</em>, Dong-Hyun Hwang, and Hideki Koike </div> <div class="periodical"> <em>IEEE Access</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ACCESS.2022.3210261" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>In the field of the acquisition of sports skills, a common way to improve sports skills, such as golf swings, is to imitate professional players’ motions. However, it is difficult for beginners to specify the keyframes on which they should focus and which part of the body they should correct because of inconsistent timing and lack of knowledge. In this study, a golf swing analysis tool using neural networks is proposed to address this gap. The proposed system compares two motion sequences and specifies keyframes in which significant differences can be observed between the two motions. In addition, the system helps users intuitively understand the differences between themselves and professional players by using interpretable clues. The main challenge of this study is to target the fine-grained differences between users and professionals that can be used for self-training. Moreover, the significance of the proposed approach is the use of an unsupervised learning method without prior knowledge and labeled data, which will benefit future applications and research in other sports and skill training processes. In our approach, neural networks are first used to create a motion synchronizer to align motions with different phases and timing. Next, a motion discrepancy detector is implemented to find fine-grained differences between motions in latent spaces that are learned by the networks. Furthermore, we consider that learning intermediate motions may be feasible for beginners because, in this way, they can gradually change their pose to match the ideal form. Therefore, based on the synchronization and discrepancy detection results, we utilize a decoder to restore the intermediate human poses between two motions from the latent space. Finally, we suggest possible applications for analyzing and visualizing the discrepancy between the two input motions and interacting with the users. With the proposed application, users can easily understand the differences between their motions and those of various experts during self-training and learn how to improve their motions.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="9974362" class="col-sm-10"> <div class="title">Virtual Club Shadow: A Real-time Projection of Golf Club Trajectory</div> <div class="author"> <em>Chen-Chieh Liao</em>, Haruki Kikuchi, Dong-Hyun Hwang, and Hideki Koike </div> <div class="periodical"> <em>In 2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)</em>, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ISMAR-Adjunct57072.2022.00177" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>In sports like golf, the motion of a club or racket determines the outcome significantly, and it is essential to consistently move such properties with the correct forms to achieve high scores during competitions. This paper proposes a novel projection method that provides real-time visualization of golf club trajectories on the ground. The system can render the trajectory of a golf club along with conventional face-to-path angle information in real-time using an optical motion capture system and a video projector. Furthermore, the system utilizes dynamic changes of color to provide more detailed information about the pose of the golf club. This color-changing method aims to help users understand their club motion trajectories and explicitly adjust their form based on the visualization results.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="10.1145/3532719.3543197" class="col-sm-10"> <div class="title">Context-aware Risk Degree Prediction for Smartphone Zombies</div> <div class="author"> Erwin Wu, <em>Chen-Chieh Liao</em>, Ruofan Liu, and Hideki Koike </div> <div class="periodical"> <em>In ACM SIGGRAPH 2022 Posters</em>, Vancouver, BC, Canada, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3532719.3543197" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Using smartphones while walking is becoming a social problem. Recent works try to support this issue by different warning systems. However, most only focus on detecting obstacles, without considering the risk to the user. In this paper, we propose a deep learning-based context-aware risk prediction system using a built-in camera on smartphones, aiming to notify ”smombies” by a risk-degree based algorithm. The proposed system both estimates the risk degree of a potential obstacle and the user’s status, which can also be used for distracted driving or visually impaired people.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="10.1145/3532719.3543196" class="col-sm-10"> <div class="title">Synchronized Hand Difference Visualization for Piano Learning</div> <div class="author"> Ruofan Liu, Erwin Wu, <em>Chen-Chieh Liao</em>, Hayato Nishioka, Shinichi Furuya, and Hideki Koike </div> <div class="periodical"> <em>In ACM SIGGRAPH 2022 Posters</em>, Vancouver, BC, Canada, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3532719.3543196" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>When learning a dexterous skill such as playing the piano, people commonly watch videos of a teacher. However, this conventional way has some downsides such as limited information to be retrieved and less intuitive instructions. We propose a virtual training system by visualizing differences between hands to provide intuitive feedback for skill acquisition. After synchronizing the data, two visual cues are proposed including a hand-overlay manner and a two-keyboards visualization. A pilot study confirm the superiority of the proposed methods over conventional video-viewing.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="10.1145/3476124.3488645" class="col-sm-10"> <div class="title">How Can I Swing Like Pro?: Golf Swing Analysis Tool for Self Training</div> <div class="author"> <em>Chen-Chieh Liao</em>, Dong-Hyun Hwang, and Hideki Koike </div> <div class="periodical"> <em>In SIGGRAPH Asia 2021 Posters</em>, Tokyo, Japan, Oct 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3476124.3488645" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>In this work, we present an analysis tool to help golf beginners compare their swing motion to the swing motion of experts. The proposed application synchronizes videos with different swing phase timings using the latent features extracted by a neural network-based encoder and detects key frames where discrepant motions occur. We visualize synchronized image frames and 3D poses that help users recognize the differences and key factors that can be important for their swing skill improvement.</p> </div> </div> </div> </li> <li> <div class="row"> <div id="10.1145/3334480.3383030" class="col-sm-10"> <div class="title">Realtime Center of Mass Adjustment via Weight Switching Device inside a Golf Putter</div> <div class="author"> <em>Chen-Chieh Liao</em>, Hideki Koike, and Takuto Nakamura </div> <div class="periodical"> <em>In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems</em>, Honolulu, HI, USA, Oct 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3334480.3383030" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>In order to improve the performance in putting, we design a weight switching device that can provide various switching angles. This paper proposes a system that can change the center of mass by switching the weight to different positions around the head of a putter. The proposed system starts switching the weight at the beginning of a downswing and ends before the putter hits a ball. To verify the effectiveness of the proposed system, we conducted a user study and examined if the face to path angle of the putter changed when the weight was switched to different positions. In the user study, the participant performed putting with different switching angles. In the analysis, we focused on the differences in the face to path angle among the switching angles. The user study showed that the proposed system is effective in changing the face to path angle when switching the weight away from the putter’s shaft. Based on the experimental results, the proposed system contributes to affecting the face to path angle dynamically in real-time.</p> </div> </div> </div> </li> </ol> </div> </div> </div> </div> </article> <div class="social"> <div class="contact-icons"> <a href="mailto:%6C%69%61%6F%63%68%65%6E%63%68%69%65%68@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/liaochenchieh" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/chen-chieh-liao" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://orcid.org/0000-0002-9850-2468" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://www.researchgate.net/profile/Chen-Chieh-Liao-2182744588/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://scholar.google.com/citations?user=jYJJazcAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> </div> <div class="contact-note">Feel free to contact me!</div> </div> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Chen-Chieh (Jacky) Liao. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>